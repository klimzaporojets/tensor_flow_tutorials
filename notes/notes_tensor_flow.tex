 \documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{courier}
\usepackage{listings}
\usepackage{color}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}

\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}

\lstset{ %
  backgroundcolor=\color{white},   % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}; should come as last argument
  basicstyle=\footnotesize,        % the size of the fonts that are used for the code
  breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
  breaklines=true,                 % sets automatic line breaking
  captionpos=b,                    % sets the caption-position to bottom
  commentstyle=\color{mygreen},    % comment style
  deletekeywords={...},            % if you want to delete keywords from the given language
  escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
  extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
  frame=single,	                   % adds a frame around the code
  keepspaces=true,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
  keywordstyle=\color{blue},       % keyword style
  language=python,                 % the language of the code
  morekeywords={*,...},           % if you want to add more keywords to the set
  numbers=left,                    % where to put the line-numbers; possible values are (none, left, right)
  numbersep=5pt,                   % how far the line-numbers are from the code
  numberstyle=\tiny\color{mygray}, % the style that is used for the line-numbers
  rulecolor=\color{black},         % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,          % underline spaces within strings only
  showtabs=false,                  % show tabs within strings adding particular underscores
  stepnumber=2,                    % the step between two line-numbers. If it's 1, each line will be numbered
  stringstyle=\color{mymauve},     % string literal style
  tabsize=2,	                   % sets default tabsize to 2 spaces
  title=\lstname                   % show the filename of files included with \lstinputlisting; also try caption instead of title
}
\author{Klim Zaporojets}
\title{Notes on udacity deep learning course}
\setlength\parindent{0pt}
\begin{document}
\section{ML For Beginners}
softmax:
\begin{enumerate}
	\item If you want to assign probabilities to an object being one of several different things, softmax is the thing to do, because softmax gives us a list of values between 0 and 1 that add up to 1. Even later on, when we train more sophisticated models, the final step will be a layer of softmax.
\end{enumerate}

$$\text{softmax}(x)_i = \dfrac{\exp(x_i)}{\sum_j{\exp(x_j)}}$$

\section{\href{www.tensorflow.org/versions/r0.11/get_started/basic_usage.html}{Get started - Basic Usage}}
%\section{\href{www.tensorflow.org/versions/r0.11/how_tos/reading_data/index.html}{Reading data}}
TensorFlow is a programming system in which you represent computations as graphs. Nodes in the graph are called ops (short for operations). An op takes zero or more \texttt{Tensors}, performs some computation, and produces zero or more \texttt{Tensors}. A \texttt{Tensor} is a typed multi-dimensional array. For example, you can represent a mini-batch of images as a 4-D array of floating point numbers with dimensions [\texttt{batch}, \texttt{height}, \texttt{width}, \texttt{channels}].\\

A TensorFlow graph is a description of computations. To compute anything, a graph must be launched in a \texttt{Session}. A \texttt{Session} places the graph ops onto \texttt{Devices}, such as CPUs or GPUs, and provides methods to execute them. These methods return tensors produced by ops as numpy \texttt{ndarray} objects in Python, and as \texttt{tensorflow::Tensor} instances in C and C++. \\

The following are the main points to understand when executing TensorFlow is how TensorFlow: 
\begin{enumerate}
	\item Represents computations as graphs: TensorFlow programs are usually structured into a construction phase, that assembles a graph, and an execution phase that uses a session to execute ops in the graph. Example of building a graph: 
	\begin{lstlisting}. Example: 
import tensorflow as tf
matrix1 = tf.constant([[3., 3.]])
matrix2 = tf.constant([[2.],[2.]])
product = tf.matmul(matrix1, matrix2)
\end{lstlisting}
The default graph now has three nodes: two \texttt{constant()} ops and one \texttt{matmul()} op. To actually multiply the matrices, and get the result of the multiplication, you must launch the graph in a session.
	\item Executes graphs in the context of \texttt{Sessions}: to launch a graph, it is necessary to create a \texttt{Session} object. Without arguments the session constructor launches the default graph. To execute the graph for the example in the previous point, we can launch the following code: 
	\begin{lstlisting}. Example: 
sess = tf.Session()
result = sess.run(product)
print(result)
sess.close() 
\end{lstlisting}
	In case of having more than one GPU available on your machine, to use a GPU beyond the first you must assign ops to it explicitly. Use \texttt{with...Device} statements to specify which CPU or GPU to use for operations. For example: 
	\begin{lstlisting}. Example: 
with tf.Session() as sess:
  with tf.device("/gpu:1"):
    matrix1 = tf.constant([[3., 3.]])
    matrix2 = tf.constant([[2.],[2.]])
    product = tf.matmul(matrix1, matrix2)
    ...
\end{lstlisting}
	Devices are specified with strings. The currently supported devices are: 
	\begin{itemize}	
		\item \texttt{"/cpu:0"}: The CPU of your machine. 
		\item \texttt{"/gpu:0"}: The GPU of your machine, if you have one. 
		\item \texttt{"/gpu:1"}: The second GPU of your machine, etc. 
	\end{itemize}
	
	To create a TensorFlow cluster, launch a TensorFlow server on each of the machines in the cluster. When you instantiate a Session in your client, you pass it the network location of one of the machines in the cluster:
	\begin{lstlisting}. Example: 
with tf.Session("grpc://example.org:2222") as sess:
  # Calls to sess.run(...) will be executed on the cluster.
  ...
	\end{lstlisting}
	This machine becomes the master for the session. The master distributes the graph across other machines in the cluster (workers), much as the local implementation distributes the graph across available compute resources within a machine.
	You can use ``with tf.device():" statements to directly specify workers for particular parts of the graph:
	\begin{lstlisting}. Example: 
with tf.device("/job:ps/task:0"):
  weights = tf.Variable(...)
  biases = tf.Variable(...)
  
  	\end{lstlisting}
The Python examples in the documentation launch the graph with a Session and use the Session.run() method to execute operations.\\
For ease of use in interactive Python environments, such as IPython you can instead use the \texttt{InteractiveSession} class, and the \texttt{Tensor.eval()} and \texttt{Operation.run()} methods. This avoids having to keep a variable holding the session.
	
	\item Represents data as tensors: TensorFlow programs use a tensor data structure to represent all data -- only tensors are passed between operations in the computation graph. You can think of a TensorFlow tensor as an n-dimensional array or list. A tensor has a static type, a rank, and a shape. 
	%here more details about it 
	\item Maintains state with \texttt{Variables}. Variables maintain state across executions of the graph. 
	
	\item Uses feeds and fetches to get data into and out of arbitrary operations. To fetch the outputs of operations, execute the graph with a \texttt{run()} call on the Session object and pass in the tensors to retrieve. The following is an example: 
\begin{lstlisting}. Example: 
input1 = tf.constant([3.0])
input2 = tf.constant([2.0])
input3 = tf.constant([5.0])
intermed = tf.add(input2, input3)
mul = tf.mul(input1, intermed)

with tf.Session() as sess:
  result = sess.run([mul, intermed])
  print(result)

# output:
# [array([ 21.], dtype=float32), array([ 7.], dtype=float32)]
\end{lstlisting}
\end{enumerate}
The examples above introduce tensors into the computation graph by storing them in \texttt{Constants} and \texttt{Variables}. TensorFlow also provides a feed mechanism for patching a tensor directly into any operation in the graph.\\
A feed temporarily replaces the output of an operation with a tensor value. You supply feed data as an argument to a \texttt{run()} call. The feed is only used for the run call to which it is passed. The most common use case involves designating specific operations to be "feed" operations by using \texttt{tf.placeholder()} to create them:
\begin{lstlisting}. Example: 
input1 = tf.placeholder(tf.float32)
input2 = tf.placeholder(tf.float32)
output = tf.mul(input1, input2)

with tf.Session() as sess:
  print(sess.run([output], feed_dict={input1:[7.], input2:[2.]}))

# output:
# [array([ 14.], dtype=float32)]
\end{lstlisting}
A \texttt{placeholder()} operation generates an error if you do not supply a feed for it
\section{Variables: Creation, Initialization, Saving, and Loading}

When you train a model, you use variables to hold and update parameters. Variables are in-memory buffers containing tensors. They must be explicitly initialized and can be saved to disk during and after training. You can later restore saved values to exercise or analyze the model.
\texttt{tf.Variable} is used to create variable. Initialization function should be provided (ex: \texttt{tf.random\_normal}) as well as the shape of the resulting tensor. 
\begin{lstlisting}. Example: 
# Create two variables.
weights = tf.Variable(tf.random_normal([784, 200], stddev=0.35),
                      name="weights")
biases = tf.Variable(tf.zeros([200]), name=``biases")
\end{lstlisting}
Variables can also be placed on a device: 
\begin{lstlisting}. Example: 
# Pin a variable to GPU.
with tf.device(``/gpu:0"):
  v = tf.Variable(...)
\end{lstlisting} 
In order to initialize variables \texttt{tf.initialize\_all\_variables()} has to be used to add op to run variables initializers on a particular session. \\
Use \texttt{initialized\_value()} property to initialize a variable from the initial value of another variable. \\
To save and restore variable values, \texttt{tf.train.Saver} object has to be used. This creates \textit{checkpoint files}. 

\section{TensorBoard}
The computations you'll use TensorFlow for - like training a massive deep neural network - can be complex and confusing. To make it easier to understand, debug, and optimize TensorFlow programs, we've included a suite of visualization tools called TensorBoard. You can use TensorBoard to visualize your TensorFlow graph, plot quantitative metrics about the execution of your graph, and show additional data like images that pass through it. \\
In order to produce logs, the data for each of the summary stats has to be summarized via \texttt{SummaryWriter} and written to the disk to a particular location that will have to be passed to the parameter \texttt{logdir} when loading tensorboard. 

To see different tensorboard  visualizations execute: \\ 
\texttt{tensorboard --logdir=path/to/log-directory}

\subsection{TensorBoard Graph visualization}
In case of dealing with lots of nodes, scoping can be an interesting option: 
\begin{lstlisting}. Example: 
import tensorflow as tf

with tf.name_scope('hidden') as scope:
  a = tf.constant(5, name='alpha')
  W = tf.Variable(tf.random_uniform([1, 2], -1.0, 1.0), name='weights')
  b = tf.Variable(tf.zeros([1]), name='biases')
  
  \end{lstlisting} 
\section{Tensorflow mechanics}
Possible to save tensorflow checkpoints using \texttt{tf.train.Saver} class.\\
\textbf{\texttt{tf.placeholder}}: inserts a placeholder for a tensor that will be always fed. 
\begin{lstlisting}. Example: 
images_placeholder = tf.placeholder(tf.float32, 
	shape=(batch_size, mnist.IMAGE_PIXELS))
labels_placeholder = tf.placeholder(tf.int32, shape=(batch_size))
\end{lstlisting}
Grouping nodes by name scopes is critical to making a legible graph. If you're building a model, name scopes give you control over the resulting visualization. \textbf{The better your name scopes, the better your visualization}.

\section{\href{www.tensorflow.org/versions/r0.11/how_tos/reading_data/index.html}{Reading data}}
There are three main methods of getting data into a TensorFlow program: 
\begin{enumerate}
	\item \textbf{Feeding}: Python code provides the data when running each step. 
	\item \textbf{Reading from files}: an input pipeline reads the data from files at the beginning of a TensorFlow graph. 
	\item \textbf{Preloaded data}: a constant or variable in the TensorFlow graph holds all the data (for small data sets). 
\end{enumerate}
\subsection{Feeding}
TensorFlow's feed mechanism lets you inject data into any Tensor in a computation graph. A python computation can thus feed data directly into the graph.\\
Supply feed data through the \texttt{feed\_dict} argument to a run() or eval() call that initiates computation.
\begin{lstlisting}. Example: 
with tf.Session():
  input = tf.placeholder(tf.float32)
  classifier = ...
  print(classifier.eval(feed_dict={input: my_python_preprocessing_fn()}))
  \end{lstlisting}

While you can replace any Tensor with feed data, including variables and constants, the best practice is to use a \texttt{\textbf{placeholder} op}  node. A placeholder exists solely to serve as the target of feeds. It is not initialized and contains no data.

\subsection{Reading from files}
A typical pipeline for reading records from files has the following stages:
\begin{enumerate}
	\item \textbf{The list of filenames}: achieved through \texttt{tf.train.string\_input\_producer} function. 
	\item \textbf{Optional filename shuffling}: achieved through \texttt{tf.train.string\_input\_producer} function.
	\item \textbf{Optional epoch limit}: achieved through \texttt{tf.train.string\_input\_producer} function.
	\item \textbf{Filename queue}: achieved through \texttt{tf.train.string\_input\_producer} function.
	\item \textbf{A Reader for the filename format}: Select the reader that matches your input file format and pass the filename queue to the reader's read method. The read method outputs a key identifying the file and record (useful for debugging if you have some weird records), and a scalar string value. Use one (or more) of the decoder and conversion ops to decode this string into the tensors that make up an example. For instance, to read text files in comma-separated value (CSV) format, use a \texttt{TextLineReader} with the \texttt{decode\_csv} operation. \\ 
	Another approach is to convert whatever data you have into a supported format. This approach makes it easier to mix and match data sets and network architectures. The recommended format for TensorFlow is a \texttt{TFRecords} file. 
	\item \textbf{A decoder for a record read by the reader}: 
	\item \textbf{Optional preprocessing}: check \href{https://www.tensorflow.org/code/tensorflow/models/image/cifar10/cifar10_input.py}{this link} for an example. 
	\item \textbf{Example queue}: the queues can be created using \texttt{tf.train.QueueRunner} class. For multi-threads queues, \texttt{tf.train.Coordinator} class should be used. 
\end{enumerate}
For \textbf{batching}, \texttt{tf.train.shuffle\_batch} function can be used. \\
If you need more parallelism or shuffling of examples between files, use multiple reader instances using the \texttt{tf.train.shuffle\_batch\_join} function. This also can be achieved with \texttt{tf.train.shuffle\_batch} function with \texttt{num\_threads} parameter bigger than 1. The advantage is that this latter method allows to perform summaries on files reading by using \texttt{tf.train.shuffle\_batch*} functions. 
\subsection{Preloaded data}
This is only used for small data sets that can be loaded entirely in memory. There are two approaches: 
\begin{enumerate}
	\item Store the data in a constant. 
	\item Store the data in a variable, that you initialize and then never change. 
\end{enumerate}
Using a constant is a bit simpler, but uses more memory (since the constant is stored inline in the graph data structure, which may be duplicated a few times).
\subsection{Multiple input pipelines}
The example in \href{https://www.tensorflow.org/versions/r0.11/tutorials/deep_cnn/index.html#save-and-restore-checkpoints}{this link} shows how to train on one dataset and evaluat (or ``eval") on another. One way to do this is to actually have two separate processes:
\begin{enumerate}
	\item The training process reads training input data and periodically writes checkpoint files with all the trained variables.
	\item The evaluation process restores the checkpoint files into an inference model that reads validation input data.
\end{enumerate}
This has a couple of benefits: 
\begin{enumerate}
	\item The eval is performed on a single snapshot of the trained variables.
	\item You can perform the eval even after training has completed and exited.
\end{enumerate}
You can have the train and eval in the same graph in the same process, and share their trained variables. See the \href{https://www.tensorflow.org/versions/r0.11/how_tos/variable_scope/index.html}{shared variables tutorial}.

\subsection{Threading and Queues}
The TensorFlow \texttt{Session} object is multithreaded, so multiple threads can easily use the same session and run ops in parallel. However, it is not always easy to implement a Python program that drives threads as described above. All threads must be able to stop together, exceptions must be caught and reported, and queues must be properly closed when stopping.

TensorFlow provides two classes to help: \texttt{tf.Coordinator} and \texttt{tf.QueueRunner}. These two classes are designed to be used together. The \texttt{Coordinator} class helps multiple threads stop together and report exceptions to a program that waits for them to stop. The \texttt{QueueRunner} class is used to create a number of threads cooperating to enqueue tensors in the same queue.

\subsubsection{Coordinator}
The Coordinator class helps multiple threads stop together. Its key methods are: 
\begin{enumerate}
\item \texttt{should\_stop()}: returns True if the threads should stop. 
\item \texttt{request\_stop(<exception>)}: requests that threads should stop. 
\item \texttt{join(<list of threads>)}: waits until the specified threads have stopped. 
\end{enumerate}
\subsubsection{QueueRunner}
The \texttt{QueueRunner} class creates a number of threads that repeatedly run an enqueue op. These threads can use a coordinator to stop together. In addition, a queue runner runs a closer thread that automatically closes the queue if an exception is reported to the coordinator.

\subsubsection{Handling Exceptions}
An important exception to get to know is \texttt{OutOfRangeError} which is used to report that a queue was closed. 

\section{\href{www.tensorflow.org/versions/r0.11/how_tos/distributed/index.html}{Distributed TensorFlow}}
The \texttt{tf.train.Server.create\_local\_server()} method creates a single-process cluster, with an in-process server:
\begin{lstlisting}. Example: 
# Start a TensorFlow server as a single-process "cluster".
import tensorflow as tf
c = tf.constant("Hello, distributed TensorFlow!")
server = tf.train.Server.create_local_server()
sess = tf.Session(server.target)  # Create a session on the server.
sess.run(c)
  \end{lstlisting}
  
\subsection{Create a cluster}  
  A TensorFlow ``cluster" is a set of ``tasks" that participate in the distributed execution of a TensorFlow graph. Each task is associated with a TensorFlow ``server", which contains a ``master" that can be used to create sessions, and a ``worker" that executes operations in the graph. A cluster can also be divided into one or more ``jobs", where each job contains one or more tasks. \\
  To create a cluster, you start one TensorFlow server per task in the cluster. Each task typically runs on a different machine, but you can run multiple tasks on the same machine (e.g. to control different GPU devices). In each task, do the following:
\begin{itemize}
	\item Create a \texttt{tf.train.ClusterSpec} that describes all of the tasks in the cluster. This should be the same for each task. Consists in a dictionary that maps job names to lists of network addresses. 
	\item Create a \texttt{tf.train.Server}, passing the \texttt{tf.train.ClusterSpec} to the constructor, and identifying the local task with a job name and task index.
	\item Finally, to place operations on a particular process, you can use the same \texttt{tf.device()} function that is used to specify whether ops run on the CPU or GPU. 

\subsection{Replicated training}
A common training configuration, called "data parallelism," involves multiple tasks in a \texttt{worker} job training the same model on different mini-batches of data, updating shared parameters hosted in a one or more tasks in a \texttt{ps} job. All tasks typically run on different machines. There are many ways to specify this structure in TensorFlow. Check online for more details about how to do it. 
\end{itemize}

\section{\href{www.tensorflow.org/versions/r0.11/how_tos/adding_an_op/index.html}{Adding a New Op}}
The online version of this section has a lot of detailed information and code examples. Here I just describe what can be done. \\
If you'd like to incorporate an operation that isn't covered by the existing library, you can create a custom Op. To incorporate your custom Op, you'll need to:
\begin{itemize}
	\item Register the new Op in a C++ file. The Op registration is independent of the implementation, and describes the semantics of how the Op is invoked. For example, it defines the Op name, and specifies its inputs and outputs. It also defines the shape function that is used for tensor shape inference.
	\item Implement the Op in C++. This implementation is called a "kernel", and there can be multiple kernels for different architectures (e.g. CPUs, GPUs) or input / output types.
	\item Optionally, create a Python wrapper. This wrapper is the public API to create the Op. A default wrapper is generated from the Op registration, which can be used directly or added to.
	\item Optionally, write a function to compute gradients for the Op.
	\item Test the Op, typically in Python. If you define gradients, you can verify them with the Python \texttt{GradientChecker}.
\end{itemize}

\section{\href{www.tensorflow.org/versions/r0.11/how_tos/new_data_formats/index.html}{Custom Data Readers}}
TensorFlow allows to write personalized readers to read data from unknown formats. These readers can be written in C++ and registered to be used in python. For more details and examples of code, read online. 
%TODO: we are here 

\section{\href{www.tensorflow.org/versions/r0.11/how_tos/using_gpu/index.html}{Using GPUs}}
On a typical system, there are multiple computing devices. In TensorFlow, the supported device types are \texttt{CPU} and \texttt{GPU}. They are represented as \texttt{strings}. For example:
\begin{itemize}
	\item \texttt{"/cpu:0"}: The CPU of your machine. 
	\item \texttt{"/gpu:0"}: The GPU of your machine, if you have one. 
	\item \texttt{"/gpu:1"}: The second GPU of your machine, etc. 
\end{itemize}
If a TensorFlow operation has both CPU and GPU implementations, the GPU devices will be given priority when the operation is assigned to a device. For example, \texttt{matmul} has both CPU and GPU kernels. On a system with devices \texttt{cpu:0} and \texttt{gpu:0}, \texttt{gpu:0} will be selected to run \texttt{matmul}.

To find out which devices your operations and tensors are assigned to, create the session with \texttt{log\_device\_placement} configuration option set to \texttt{True}.
\begin{lstlisting}. Example2: 
# Creates a graph.
a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')
c = tf.matmul(a, b)
# Creates a session with log_device_placement set to True.
sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
# Runs the op.
print sess.run(c)
  \end{lstlisting}
If you would like a particular operation to run on a device of your choice instead of what's automatically selected for you, you can use with \texttt{tf.device} to create a device context such that all the operations within that context will have the same device assignment: 
\begin{lstlisting}. Example: 
# Creates a graph.
with tf.device('/cpu:0'):
  a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
  b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')
  c = tf.matmul(a, b)
# Creates a session with log_device_placement set to True.
sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
# Runs the op.
print sess.run(c)
  \end{lstlisting}

Different GPU memory allocations methods exist, those can be configured inside \texttt{config.gpu\_options}. \\
If you would like to run TensorFlow on multiple GPUs, you can construct your model in a multi-tower ffashion where each tower is assigned to a different GPU. For example: 
\begin{lstlisting}. Example: 
# Creates a graph.
c = []
for d in ['/gpu:2', '/gpu:3']:
  with tf.device(d):
    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3])
    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2])
    c.append(tf.matmul(a, b))
with tf.device('/cpu:0'):
  sum = tf.add_n(c)
# Creates a session with log_device_placement set to True.
sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
# Runs the op.
print sess.run(sum)
  \end{lstlisting}
The \href{https://www.tensorflow.org/versions/r0.11/tutorials/deep_cnn/index.html}{cifrar10 tutorial} is a good example demonstrating how to do training with multiple GPUs. 

\section{\href{www.tensorflow.org/versions/r0.11/how_tos/variable_scope/index.html}{Sharing Variables}}
When building complex models you often need to share large sets of variables and you might want to initialize all of them in one place. The following two methods are being used: 
\begin{itemize}
	\item \texttt{tf.variable\_scope(<name>, <shape>, <initializer>)}: creates or returns a variable with a given name.
	\item \texttt{tf.get\_variable(<scope\_name>))}: manages namespaces for names passed to \texttt{tf.get\_variable()}. 
\end{itemize}
The function \texttt{tf.get\_variable()} is used to get or create variable instead of a direct call to \texttt{tf.Variable}. It uses an initializer instead of passing the value directly, as in \texttt{tf.Variable}. Several initializers are available inside \texttt{tf} package and can be read online. \\
In order to re-use variables in different scopes (ex: for different network layers), \texttt{tf.variable\_scope()} should be used. Example: 
\begin{lstlisting}. Example: 
def conv_relu(input, kernel_shape, bias_shape):
    # Create variable named "weights".
    weights = tf.get_variable("weights", kernel_shape,
        initializer=tf.random_normal_initializer())
    # Create variable named "biases".
    biases = tf.get_variable("biases", bias_shape,
        initializer=tf.constant_initializer(0.0))
    conv = tf.nn.conv2d(input, weights,
        strides=[1, 1, 1, 1], padding='SAME')
    return tf.nn.relu(conv + biases)
    
def my_image_filter(input_images):
    with tf.variable_scope("conv1"):
        # Variables created here will be named "conv1/weights", "conv1/biases".
        relu1 = conv_relu(input_images, [5, 5, 32, 32], [32])
    with tf.variable_scope("conv2"):
        # Variables created here will be named "conv2/weights", "conv2/biases".
        return conv_relu(relu1, [5, 5, 32, 32], [32])   
  \end{lstlisting}
  Much more options on variables scopes and re-usability are available and can be read online. Some of the following are the examples to check the code: 
  \begin{itemize}
  	\item \texttt{models/image/cifrar10.py}: model for detecting object in images. 
  	\item \texttt{models/rnn/rnn\_cell.py}: cell functions for recurrent neural networks. 
  	\item \texttt{models/rnn/seq2seq.py}: functions for building sequence-to-sequence models. 
  \end{itemize}
  
  \section{\href{www.tensorflow.org/versions/r0.11/how_tos/hadoop/index.html}{How to run TensorFlow on Hadoop}}
  Basically, it is possible to use HDFS with TensorFlow by changing paths that are used to read and write data to an HDFS path by using \texttt{tf.train.string\_input\_producer}. 
  
  \section{\href{www.tensorflow.org/versions/r0.11/how_tos/language_bindings/index.html}{TensorFlow in other languages}}
  Python was the first client language supported by TensorFlow and currently supports the most features. More and more of that functionality is being moved into the core of TensorFlow (implemented in C++) and exposed via a \href{https://www.tensorflow.org/code/tensorflow/c/c_api.h}{C API}. Client languages should use the language's \href{https://en.wikipedia.org/wiki/Foreign_function_interface}{foreign function interface (FFI)} ti call into this C API to provide TensorFlow functionality. \\ 
  Providing TensorFlow functionality in a programming language can be broken down into broad categories: 
  \begin{itemize}
  	\item \textbf{Run a predefined graph}: Given a \texttt{GraphDEf} (or \texttt{MetaGraphDef}) protocol message, be able to create a session, run queries, and get tensor results. This is sufficient for a mobile app or server that wants to run inference on a pre-trained model. 
  	\item \textbf{Graph construction}: 
  \end{itemize}
\end{document}